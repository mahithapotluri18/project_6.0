{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b670c9c",
   "metadata": {},
   "source": [
    "#  ClimateScope — Semi-Real-Time Weather Analysis (Milestone 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b943c08",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66f43ebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working dir: g:\\project_6.0\n"
     ]
    }
   ],
   "source": [
    "import os, json, time, subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "DATA = Path('data')\n",
    "for sub in ['raw', 'clean', 'tmp']:\n",
    "    (DATA / sub).mkdir(parents=True, exist_ok=True)\n",
    "print('Working dir:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8574c",
   "metadata": {},
   "source": [
    "## Kaggle Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1bceb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kaggle credentials: OK\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "# Try env vars first, else fallback to kaggle (1).json\n",
    "user = os.environ.get('KAGGLE_USERNAME')\n",
    "key = os.environ.get('KAGGLE_KEY')\n",
    "if not (user and key):\n",
    "    try:\n",
    "        with open(r'c:\\Users\\Dell\\Downloads\\kaggle (1).json') as f:\n",
    "            creds = json.load(f)\n",
    "            user = creds.get('username')\n",
    "            key = creds.get('key')\n",
    "    except Exception as e:\n",
    "        print('Could not read kaggle (1).json:', e)\n",
    "if user and key:\n",
    "    kaggle_json = {'username': user, 'key': key}\n",
    "    kaggle_path = Path.home() / '.kaggle'\n",
    "    kaggle_path.mkdir(exist_ok=True)\n",
    "    with open(kaggle_path / 'kaggle.json', 'w') as f: json.dump(kaggle_json, f)\n",
    "    try: os.chmod(kaggle_path / 'kaggle.json', 0o600)\n",
    "    except Exception: pass\n",
    "print('Kaggle credentials:', 'OK' if user and key else 'Missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d11e03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: nelgiriyewithana/global-weather-repository\n"
     ]
    }
   ],
   "source": [
    "# Dataset and path configuration\n",
    "KAGGLE_SLUG = 'nelgiriyewithana/global-weather-repository'\n",
    "RAW = DATA / 'raw'\n",
    "CLEAN = DATA / 'clean'\n",
    "TMP = DATA / 'tmp'\n",
    "print(f'Config: {KAGGLE_SLUG}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcbe1c3",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6714f53c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV path: data\\raw\\sample.csv\n"
     ]
    }
   ],
   "source": [
    "# Find latest CSV in data/raw/** or create sample\n",
    "csvs = list(RAW.glob('**/*.csv'))\n",
    "if not csvs:\n",
    "    sample = RAW / 'sample.csv'\n",
    "    sample.write_text('date,country,temperature_c,humidity,precipitation_mm,wind_speed_kmh\\n' + '\\n'.join([f'2025-08-{d:02d},Country{c},{20+c+d},{60+d},{5+d},{10+d}' for c in range(1,4) for d in range(1,8)]))\n",
    "    csv_path = sample\n",
    "else:\n",
    "    csv_path = max(csvs, key=lambda p: p.stat().st_mtime)  # latest by modification time\n",
    "print('CSV path:', csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b879206",
   "metadata": {},
   "source": [
    "## Locate csv_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b4a9bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ csv_path defined: data\\raw\\sample.csv\n",
      "✓ File exists: True\n",
      "✓ Milestone 1 Part 1 complete - ready for automation logic\n"
     ]
    }
   ],
   "source": [
    "# Confirm csv_path is ready for next steps\n",
    "print(f'✓ csv_path defined: {csv_path}')\n",
    "print(f'✓ File exists: {csv_path.exists()}')\n",
    "print('✓ Milestone 1 Part 1 complete - ready for automation logic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9ff3d",
   "metadata": {},
   "source": [
    "## Automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "679d6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automation setup: epoch-based\n"
     ]
    }
   ],
   "source": [
    "# Row-based trigger using last_updated_epoch\n",
    "force_refresh = False  # set True to force\n",
    "epoch_file = TMP / 'last_epoch.txt'\n",
    "need_refresh = force_refresh\n",
    "# Will check actual epochs after loading data\n",
    "print('Automation setup:', 'force' if force_refresh else 'epoch-based')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f40287",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50f9785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skip: No refresh needed\n"
     ]
    }
   ],
   "source": [
    "# Conditional download - only if need_refresh=True\n",
    "if need_refresh:\n",
    "    try:\n",
    "        subprocess.run(['kaggle', 'datasets', 'download', '-d', KAGGLE_SLUG, '-p', str(RAW), '--force'], check=True)\n",
    "        print('✓ Fresh data downloaded from Kaggle')\n",
    "    except Exception as e:\n",
    "        print(f'Skip: Download failed ({e}), using existing data')\n",
    "else:\n",
    "    print('Skip: No refresh needed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c1fbf0",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4acc208a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: (21, 6), need_refresh: False\n"
     ]
    }
   ],
   "source": [
    "# Read CSV and update automation trigger\n",
    "df = pd.read_csv(csv_path)\n",
    "epoch_col = 'last_updated_epoch' if 'last_updated_epoch' in df.columns else None\n",
    "if epoch_col and not force_refresh:\n",
    "    latest_epoch = str(df[epoch_col].max())\n",
    "    prev_epoch = epoch_file.read_text().strip() if epoch_file.exists() else ''\n",
    "    if latest_epoch != prev_epoch:\n",
    "        epoch_file.write_text(latest_epoch)\n",
    "print(f'Data loaded: {df.shape}, need_refresh: {need_refresh}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032c41d5",
   "metadata": {},
   "source": [
    "## Column Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f10418b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped - Date: date, Region: country, Numeric: 4 cols\n"
     ]
    }
   ],
   "source": [
    "# Identify key columns\n",
    "date_col = 'date' if 'date' in df.columns else df.columns[0]\n",
    "region_col = 'country' if 'country' in df.columns else df.columns[1]\n",
    "num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "print(f'Mapped - Date: {date_col}, Region: {region_col}, Numeric: {len(num_cols)} cols')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "465a6197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NA counts: {}\n"
     ]
    }
   ],
   "source": [
    "# Quick null check\n",
    "if len(df) > 0:\n",
    "    key_cols = [date_col, region_col] + num_cols\n",
    "    na_summary = df[key_cols].isna().sum()\n",
    "    print('NA counts:', dict(na_summary[na_summary > 0]))\n",
    "else:\n",
    "    print('Skip: No data loaded')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3926c19c",
   "metadata": {},
   "source": [
    "## Date Casting & Light Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91856fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean shape: (21, 6)\n"
     ]
    }
   ],
   "source": [
    "# Date conversion and basic cleaning\n",
    "df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "df = df.dropna(subset=[date_col, region_col])\n",
    "# Light numeric handling\n",
    "if 'humidity' in num_cols:\n",
    "    df['humidity'] = df['humidity'].clip(0, 100)\n",
    "print(f'Clean shape: {df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a817cee1",
   "metadata": {},
   "source": [
    "## Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a1a3700f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monthly aggregation: (3, 6), columns: ['month', 'country', 'temperature_c', 'humidity', 'precipitation_mm', 'wind_speed_kmh']\n"
     ]
    }
   ],
   "source": [
    "# Create monthly DataFrame with normalized month column\n",
    "month = df[date_col].dt.to_period('M').dt.to_timestamp()\n",
    "monthly = df.groupby([month, region_col])[num_cols].mean(numeric_only=True).reset_index()\n",
    "# Normalize month column name\n",
    "monthly = monthly.rename(columns={monthly.columns[0]: 'month'})\n",
    "print(f'Monthly aggregation: {monthly.shape}, columns: {list(monthly.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ada62",
   "metadata": {},
   "source": [
    "## Save to Partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3628b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Partitioned data saved to data\\clean\n"
     ]
    }
   ],
   "source": [
    "# Save to Parquet partitions by year/month with per-region files\n",
    "df['year'] = df[date_col].dt.year\n",
    "df['month_num'] = df[date_col].dt.month\n",
    "for (y, m), group in df.groupby(['year', 'month_num']):\n",
    "    outdir = CLEAN / f'{y}/{m:02d}'\n",
    "    outdir.mkdir(parents=True, exist_ok=True)\n",
    "    main_file = outdir / 'clean.parquet'\n",
    "    if not main_file.exists():\n",
    "        group.to_parquet(main_file)\n",
    "print(f'✓ Partitioned data saved to {CLEAN}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
